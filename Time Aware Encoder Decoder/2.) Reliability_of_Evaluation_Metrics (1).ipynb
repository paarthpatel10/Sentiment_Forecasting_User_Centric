{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Metric Reliability Test: To ensure we selected stable and\n",
        "meaningful metrics for evaluating sentiment forecasting, we\n",
        "conducted a reliability test inspired by the methodology of\n",
        "St-Aubin and Agard (2022). This test evaluates how consis-\n",
        "tently each metric behaves when forecasting errors are intro-\n",
        "duced under controlled conditions.\n",
        "We simulated sentiment forecasting outputs by adding\n",
        "synthetic noise to real sentiment sequences. The noise in-\n",
        "cluded varying bias levels (from −0.2 to 0.2) and variance\n",
        "(from 0.01 to 0.5), reflecting realistic forecasting errors like\n",
        "shifts in opinion or fluctuation in engagement. Each metric\n",
        "was then evaluated over multiple runs to compute:\n",
        "• Variability: Standard deviation of the metric values\n",
        "across noise levels. Lower variability indicates the met-\n",
        "ric remains stable when predictions are noisy.\n",
        "• Confidence Interval Width (CI Width): The width of\n",
        "the 95% confidence interval around the metric’s aver-\n",
        "age score. Narrower intervals indicate greater precision\n",
        "in evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjiM04UKsmmt",
        "outputId": "788f34c6-b1a9-4ba0-dcdc-5129a4a81c24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metric Reliability (Lower variability and CI width = more reliable):\n",
            "MSE: Variability = 0.1296, CI Width = 0.1544\n",
            "RMSE: Variability = 0.1747, CI Width = 0.2173\n",
            "MAE: Variability = 0.1333, CI Width = 0.1537\n",
            "sRMSE: Variability = 0.3247, CI Width = 0.4015\n",
            "MASE: Variability = 0.3112, CI Width = 0.3709\n",
            "sPIS: Variability = 0.2676, CI Width = 0.3380\n",
            "sAPIS: Variability = 0.3014, CI Width = 0.3389\n",
            "SMAPE: Variability = 23.1660, CI Width = 26.1985\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def load_sentiment_data(file_path):\n",
        "    data = pd.read_csv(file_path)\n",
        "   \n",
        "    data['sentiment_score'] = data['sentiment_score'].clip(-1, 1)\n",
        "    return data\n",
        "\n",
        "\n",
        "def MSE(y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred)\n",
        "\n",
        "def RMSE(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def MAE(y_true, y_pred):\n",
        "    return np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "def sRMSE(y_true, y_pred, scale):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    return rmse / scale\n",
        "\n",
        "def MASE(y_true, y_pred, y_naive):\n",
        "    errors = np.abs(y_true - y_pred)\n",
        "    naive_errors = np.abs(y_true - y_naive)\n",
        "    return np.mean(errors) / np.mean(naive_errors[naive_errors != 0] + 1e-10)\n",
        "\n",
        "def sPIS(y_true, y_pred, scale):\n",
        "    errors = y_pred - y_true\n",
        "    return np.mean(errors) / scale  \n",
        "\n",
        "def sAPIS(y_true, y_pred, scale):\n",
        "    errors = y_pred - y_true\n",
        "    return (np.mean(np.abs(errors)) + np.abs(np.mean(errors))) / scale \n",
        "\n",
        "def SMAPE(y_true, y_pred):\n",
        "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-10))\n",
        "\n",
        "def test_metrics(file_path, n_samples=None):\n",
        "    \n",
        "    data = load_sentiment_data(file_path)\n",
        "    y_true = data['sentiment_score'].values\n",
        "    if n_samples:\n",
        "        y_true = y_true[:n_samples] \n",
        "    scale = np.std(y_true) if np.std(y_true) > 0 else 1\n",
        "   \n",
        "    y_naive = np.roll(y_true, 1)\n",
        "    y_naive[0] = y_true[0]  \n",
        "\n",
        "    bias_values = [-0.2, 0, 0.2]  \n",
        "    variance_values = [0.01, 0.1, 0.5]  \n",
        "    results = []\n",
        "\n",
        "    for bias in bias_values:\n",
        "        for var in variance_values:\n",
        "            \n",
        "            noise = np.random.normal(bias, np.sqrt(var), len(y_true))\n",
        "            y_pred = np.clip(y_true + noise, -1, 1)\n",
        "\n",
        "        \n",
        "            metrics = {\n",
        "                \"bias\": bias,\n",
        "                \"var\": var,\n",
        "                \"MSE\": MSE(y_true, y_pred),\n",
        "                \"RMSE\": RMSE(y_true, y_pred),\n",
        "                \"MAE\": MAE(y_true, y_pred),\n",
        "                \"sRMSE\": sRMSE(y_true, y_pred, scale),\n",
        "                \"MASE\": MASE(y_true, y_pred, y_naive),\n",
        "                \"sPIS\": sPIS(y_true, y_pred, scale),\n",
        "                \"sAPIS\": sAPIS(y_true, y_pred, scale),\n",
        "                \"SMAPE\": SMAPE(y_true, y_pred)\n",
        "            }\n",
        "            results.append(metrics)\n",
        "\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    reliability = {}\n",
        "    for metric in [\"MSE\", \"RMSE\", \"MAE\", \"sRMSE\", \"MASE\", \"sPIS\", \"sAPIS\", \"SMAPE\"]:\n",
        "     \n",
        "        variability = results_df[metric].std()\n",
        "\n",
        "        boot_values = []\n",
        "        for _ in range(100):\n",
        "            sample = results_df[metric].sample(frac=1, replace=True)\n",
        "            boot_values.append(sample.mean())\n",
        "        ci = np.percentile(boot_values, [2.5, 97.5])\n",
        "        ci_width = ci[1] - ci[0]\n",
        "\n",
        "        reliability[metric] = {\"variability\": variability, \"ci_width\": ci_width}\n",
        "\n",
        "\n",
        "    print(\"Metric Reliability (Lower variability and CI width = more reliable):\")\n",
        "    for metric, stats in reliability.items():\n",
        "        print(f\"{metric}: Variability = {stats['variability']:.4f}, CI Width = {stats['ci_width']:.4f}\")\n",
        "\n",
        "    for metric in [\"MSE\", \"RMSE\", \"MAE\", \"sRMSE\", \"MASE\", \"sPIS\", \"sAPIS\", \"SMAPE\"]:\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.scatter(results_df[\"bias\"], results_df[metric], alpha=0.5)\n",
        "        plt.title(f\"{metric} Sensitivity to Bias\")\n",
        "        plt.xlabel(\"Bias\")\n",
        "        plt.ylabel(metric)\n",
        "        plt.savefig(f\"{metric}_sensitivity.png\")\n",
        "        plt.close()\n",
        "\n",
        "    return results_df, reliability\n",
        "\n",
        "results_df, reliability = test_metrics('/content/Score_output_data.csv', n_samples=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results of the test (variability, CI width):\n",
        "• MSE: most reliable, especially for penalizing large errors.\n",
        "\n",
        "• MAE: nearly as reliable as MSE; intuitive and robust.\n",
        "\n",
        "• RMSE: reliable and highlights large deviations.\n",
        "\n",
        "• sPIS: slightly less stable but valuable for identifying bias.\n",
        "\n",
        "This evaluation ensures our chosen metrics are reliable,interpretable, and suitable for forecasting sentiment scores\n",
        "over irregular time steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
