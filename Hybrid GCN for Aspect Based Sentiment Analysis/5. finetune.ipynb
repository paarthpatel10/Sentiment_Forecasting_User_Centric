{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell sets up the environment for fine-tuning by importing libraries, configuring random seeds, and enabling GPU if available. It ensures the data directory exists, initializes spaCy for NLP tasks, and loads the RoBERTa model and tokenizer for generating embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA L40S\n",
      "PyTorch CUDA version: 11.7\n",
      "Number of CUDA devices: 1\n",
      "Ensured data/ directory exists\n",
      "PyTorch Lightning version: 2.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa model loaded successfully on cuda.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pytorch_lightning as pl\n",
    "from torch_geometric.nn import GCNConv\n",
    "import random\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch_geometric.data.collate\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.modules.loss\")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(0)\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"PyTorch CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"Using CPU for fine-tuning.\")\n",
    "os.makedirs('data', exist_ok=True)\n",
    "print(\"Ensured data/ directory exists\")\n",
    "print(\"PyTorch Lightning version:\", pl.__version__)\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['ner', 'lemmatizer'])\n",
    "    tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "    roberta_model = AutoModel.from_pretrained('roberta-base').to(device)\n",
    "    print(f\"RoBERTa model loaded successfully on {device}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load spaCy or RoBERTa: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell loads dependency types from dep_types.pkl for graph construction and the SemEval ABSA dataset. It validates the dataset for required columns (sentence, aspect, sentiment) and prints sample data to ensure proper loading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Loading ABSA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dep_types and dep_type_to_idx (num_dep_types: 44)\n",
      "Loaded train_df with 160 samples\n",
      "Sample of train_df:\n",
      "[{\"sentence\":\"But the staff was so horrible to us.\",\"aspect\":\"staff\",\"sentiment\":\"negative\"},{\"sentence\":\"To be completely fair, the only redeeming factor was the food, which was above average, but couldn't make up for all the other deficiencies of Teodora.\",\"aspect\":\"food\",\"sentiment\":\"positive\"},{\"sentence\":\"The food is uniformly exceptional, with a very capable kitchen which will proudly whip up whatever you feel like eating, whether it's on the menu or not.\",\"aspect\":\"food\",\"sentiment\":\"positive\"},{\"sentence\":\"The food is uniformly exceptional, with a very capable kitchen which will proudly whip up whatever you feel like eating, whether it's on the menu or not.\",\"aspect\":\"kitchen\",\"sentiment\":\"positive\"},{\"sentence\":\"The food is uniformly exceptional, with a very capable kitchen which will proudly whip up whatever you feel like eating, whether it's on the menu or not.\",\"aspect\":\"menu\",\"sentiment\":\"neutral\"}]\n",
      "Loaded val_df with 40 samples\n",
      "Sample of val_df:\n",
      "[{\"sentence\":\"But the staff was so horrible to us.\",\"aspect\":\"staff\",\"sentiment\":\"negative\"},{\"sentence\":\"To be completely fair, the only redeeming factor was the food, which was above average, but couldn't make up for all the other deficiencies of Teodora.\",\"aspect\":\"food\",\"sentiment\":\"positive\"},{\"sentence\":\"The food is uniformly exceptional, with a very capable kitchen which will proudly whip up whatever you feel like eating, whether it's on the menu or not.\",\"aspect\":\"food\",\"sentiment\":\"positive\"},{\"sentence\":\"The food is uniformly exceptional, with a very capable kitchen which will proudly whip up whatever you feel like eating, whether it's on the menu or not.\",\"aspect\":\"kitchen\",\"sentiment\":\"positive\"},{\"sentence\":\"The food is uniformly exceptional, with a very capable kitchen which will proudly whip up whatever you feel like eating, whether it's on the menu or not.\",\"aspect\":\"menu\",\"sentiment\":\"neutral\"}]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open('data/dep_types.pkl', 'rb') as f:\n",
    "        dep_data = pickle.load(f)\n",
    "    dep_types = dep_data['dep_types']\n",
    "    dep_type_to_idx = dep_data['dep_type_to_idx']\n",
    "    print(f\"Loaded dep_types and dep_type_to_idx (num_dep_types: {len(dep_types)})\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Dependency types file not found. Ensure 'data/dep_types.pkl' exists from pre-training.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    train_df = pd.read_csv('data/semeval_train.csv')\n",
    "    val_df = pd.read_csv('data/semeval_val.csv')\n",
    "    required_columns = ['sentence', 'aspect', 'sentiment']\n",
    "    for df, name in [(train_df, 'train_df'), (val_df, 'val_df')]:\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"Error: Missing required columns in {name}: {missing_columns}\")\n",
    "            exit()\n",
    "        df = df[df['sentence'].notna() & (df['sentence'].str.strip() != '') & df['aspect'].notna()]\n",
    "        print(f\"Loaded {name} with {len(df)} samples\")\n",
    "        print(f\"Sample of {name}:\")\n",
    "        print(df.head().to_json(orient='records'))\n",
    "        if df.empty:\n",
    "            raise ValueError(f\"No valid data found in {name} after cleaning.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Dataset file not found: {e}. Ensure 'data/semeval_train.csv' and 'data/semeval_val.csv' exist.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell maps sentiment labels to indices, computes class weights, and generates dependency and contextual graphs for the ABSA dataset. It creates RoBERTa embeddings, identifies aspect nodes, and saves the train and validation graphs to pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Some sentiment labels in dataset could not be mapped.\n",
      "                                              sentence sentiment\n",
      "65   The food was delicious but do not come here on...  conflict\n",
      "102  The service varys from day to day- sometimes t...  conflict\n",
      "134  Though the Spider Roll may look like a challen...  conflict\n",
      "143  An oasis of refinement:  Food, though somewhat...  conflict\n",
      "Dataset size after dropping unmapped sentiments: 156\n",
      "Class distribution in train_df:\n",
      "0.0    99\n",
      "1.0    27\n",
      "2.0    30\n",
      "Name: sentiment_label, dtype: int64\n",
      "Class distribution in val_df:\n",
      "0    20\n",
      "1     8\n",
      "2    12\n",
      "Name: sentiment_label, dtype: int64\n",
      "Class weights: tensor([0.3766, 1.3808, 1.2427], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Aspect 'interior deco' not found in sentence 'this little place has a cute interior decor and affordable city prices.'.\n",
      "Warning: Aspect 'wine by the glass' not found in sentence 'Wine list selection is good and wine-by-the-glass was generously filled to the top.'.\n",
      "Warning: Aspect 'pre theater menu' not found in sentence 'Le Pere Pinard has a $15 pre-theater menu that is outstanding.'.\n",
      "Warning: Aspect 'congee (rice porridge)' not found in sentence 'I also recommend the rice dishes or the different varieties of congee (rice porridge).'.\n",
      "Total train graphs: 156, Total validation graphs: 40\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentiment_to_idx = {'positive': 0, 'negative': 1, 'neutral': 2}\n",
    "idx_to_sentiment = {idx: label for label, idx in sentiment_to_idx.items()}\n",
    "for df in [train_df, val_df]:\n",
    "    df['sentiment_label'] = df['sentiment'].map(sentiment_to_idx)\n",
    "    if df['sentiment_label'].isna().any():\n",
    "        print(f\"Warning: Some sentiment labels in {df.name if hasattr(df, 'name') else 'dataset'} could not be mapped.\")\n",
    "        print(df[df['sentiment_label'].isna()][['sentence', 'sentiment']])\n",
    "        df = df.dropna(subset=['sentiment_label'])\n",
    "        print(f\"Dataset size after dropping unmapped sentiments: {len(df)}\")\n",
    "\n",
    "print(\"Class distribution in train_df:\")\n",
    "class_counts = train_df['sentiment_label'].value_counts().sort_index()\n",
    "print(class_counts)\n",
    "print(\"Class distribution in val_df:\")\n",
    "print(val_df['sentiment_label'].value_counts().sort_index())\n",
    "class_weights = torch.tensor([1.0 / class_counts[i] for i in range(len(class_counts))], dtype=torch.float).to(device)\n",
    "class_weights = class_weights / class_weights.sum() * len(class_counts)\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "def get_roberta_embeddings(sentence, nodes):\n",
    "    sentences = [sentence]\n",
    "    nodes_list = [nodes]\n",
    "    valid_sentences = []\n",
    "    valid_nodes = []\n",
    "    valid_indices = []\n",
    "    for i, (sent, node) in enumerate(zip(sentences, nodes_list)):\n",
    "        if isinstance(sent, str) and sent.strip() and node:\n",
    "            valid_sentences.append(sent)\n",
    "            valid_nodes.append(node)\n",
    "            valid_indices.append(i)\n",
    "    if not valid_sentences:\n",
    "        return None, 0\n",
    "    try:\n",
    "        inputs = tokenizer(valid_sentences, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = roberta_model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.cpu()\n",
    "        num_nodes = min(len(valid_nodes[0]), embeddings[0].shape[0] - 2)\n",
    "        if num_nodes == 0:\n",
    "            print(f\"Warning: No valid nodes for sentence {valid_sentences[0]}\")\n",
    "            return None, 0\n",
    "        emb = embeddings[0, 1:num_nodes+1].clone()\n",
    "        return emb, num_nodes\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error in RoBERTa embeddings: {e}\")\n",
    "        return None, 0\n",
    "\n",
    "def create_dependency_graph(sentence, num_nodes=None):\n",
    "    doc = nlp(sentence)\n",
    "    original_nodes = [token.text.lower() for token in doc]\n",
    "    if num_nodes is None:\n",
    "        num_nodes = len(original_nodes)\n",
    "    nodes = original_nodes[:num_nodes]\n",
    "    edges = []\n",
    "    edge_types = []\n",
    "    for token in doc:\n",
    "        if token.head != token and token.i < num_nodes and token.head.i < num_nodes:\n",
    "            edges.append([token.i, token.head.i])\n",
    "            edge_types.append(dep_type_to_idx.get(token.dep_, 0))\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous() if edges else torch.empty((2, 0), dtype=torch.long)\n",
    "    edge_types = torch.tensor(edge_types, dtype=torch.long) if edge_types else torch.empty((0,), dtype=torch.long)\n",
    "    noun_indices = [i for i, token in enumerate(doc) if token.pos_ == 'NOUN' and i < num_nodes]\n",
    "    adj_indices = [i for i, token in enumerate(doc) if token.pos_ == 'ADJ' and i < num_nodes]\n",
    "    graph = Data(edge_index=edge_index, edge_attr=edge_types, sentence=sentence, nodes=nodes, noun_indices=noun_indices, adj_indices=adj_indices)\n",
    "    return graph\n",
    "\n",
    "def create_contextual_graph(sentence, num_nodes=None):\n",
    "    doc = nlp(sentence)\n",
    "    original_nodes = [token.text.lower() for token in doc]\n",
    "    if num_nodes is None:\n",
    "        num_nodes = len(original_nodes)\n",
    "    nodes = original_nodes[:num_nodes]\n",
    "    edges = []\n",
    "    window = 2\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(max(0, i - window), min(num_nodes, i + window + 1)):\n",
    "            if i != j and i < num_nodes and j < num_nodes:\n",
    "                edges.append([i, j])\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous() if edges else torch.empty((2, 0), dtype=torch.long)\n",
    "    noun_indices = [i for i, token in enumerate(doc) if token.pos_ == 'NOUN' and i < num_nodes]\n",
    "    adj_indices = [i for i, token in enumerate(doc) if token.pos_ == 'ADJ' and i < num_nodes]\n",
    "    graph = Data(edge_index=edge_index, sentence=sentence, nodes=nodes, noun_indices=noun_indices, adj_indices=adj_indices)\n",
    "    return graph\n",
    "\n",
    "def normalize_tokens(text):\n",
    "    return ' '.join(text.lower().replace('-', ' ').split())\n",
    "\n",
    "train_graphs = []\n",
    "for _, row in train_df.iterrows():\n",
    "    dep_graph = create_dependency_graph(row['sentence'])\n",
    "    if dep_graph is None:\n",
    "        continue\n",
    "    context_graph = create_contextual_graph(row['sentence'])\n",
    "    if context_graph is None:\n",
    "        continue\n",
    "    embeddings, num_nodes = get_roberta_embeddings(row['sentence'], dep_graph.nodes)\n",
    "    if embeddings is None:\n",
    "        continue\n",
    "    dep_graph = create_dependency_graph(row['sentence'], num_nodes)\n",
    "    context_graph = create_contextual_graph(row['sentence'], num_nodes)\n",
    "    dep_graph.x = embeddings\n",
    "    context_graph.x = embeddings\n",
    "    aspect = normalize_tokens(row['aspect'])\n",
    "    aspect_tokens = aspect.split()\n",
    "    aspect_node_idx = None\n",
    "    nodes = [normalize_tokens(node) for node in dep_graph.nodes]\n",
    "    for i in range(len(nodes) - len(aspect_tokens) + 1):\n",
    "        if ' '.join(nodes[i:i + len(aspect_tokens)]) == aspect:\n",
    "            aspect_node_idx = i\n",
    "            break\n",
    "    if aspect_node_idx is None:\n",
    "        print(f\"Warning: Aspect '{aspect}' not found in sentence '{row['sentence']}'.\")\n",
    "        continue\n",
    "    dep_graph.aspect_node_idx = aspect_node_idx\n",
    "    context_graph.aspect_node_idx = aspect_node_idx\n",
    "    dep_graph.aspect = row['aspect']\n",
    "    context_graph.aspect = row['aspect']\n",
    "    dep_graph.label = row['sentiment_label']\n",
    "    context_graph.label = row['sentiment_label']\n",
    "    train_graphs.append((context_graph, dep_graph))\n",
    "\n",
    "val_graphs = []\n",
    "for _, row in val_df.iterrows():\n",
    "    dep_graph = create_dependency_graph(row['sentence'])\n",
    "    if dep_graph is None:\n",
    "        continue\n",
    "    context_graph = create_contextual_graph(row['sentence'])\n",
    "    if context_graph is None:\n",
    "        continue\n",
    "    embeddings, num_nodes = get_roberta_embeddings(row['sentence'], dep_graph.nodes)\n",
    "    if embeddings is None:\n",
    "        continue\n",
    "    dep_graph = create_dependency_graph(row['sentence'], num_nodes)\n",
    "    context_graph = create_contextual_graph(row['sentence'], num_nodes)\n",
    "    dep_graph.x = embeddings\n",
    "    context_graph.x = embeddings\n",
    "    aspect = normalize_tokens(row['aspect'])\n",
    "    aspect_tokens = aspect.split()\n",
    "    aspect_node_idx = None\n",
    "    nodes = [normalize_tokens(node) for node in dep_graph.nodes]\n",
    "    for i in range(len(nodes) - len(aspect_tokens) + 1):\n",
    "        if ' '.join(nodes[i:i + len(aspect_tokens)]) == aspect:\n",
    "            aspect_node_idx = i\n",
    "            break\n",
    "    if aspect_node_idx is None:\n",
    "        print(f\"Warning: Aspect '{aspect}' not found in sentence '{row['sentence']}'.\")\n",
    "        continue\n",
    "    dep_graph.aspect_node_idx = aspect_node_idx\n",
    "    context_graph.aspect_node_idx = aspect_node_idx\n",
    "    dep_graph.aspect = row['aspect']\n",
    "    context_graph.aspect = row['aspect']\n",
    "    dep_graph.label = row['sentiment_label']\n",
    "    context_graph.label = row['sentiment_label']\n",
    "    val_graphs.append((context_graph, dep_graph))\n",
    "\n",
    "with open('data/semeval_train_graphs_embedded.pkl', 'wb') as f:\n",
    "    pickle.dump(train_graphs, f)\n",
    "with open('data/semeval_val_graphs_embedded.pkl', 'wb') as f:\n",
    "    pickle.dump(val_graphs, f)\n",
    "print(f\"Total train graphs: {len(train_graphs)}, Total validation graphs: {len(val_graphs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines a custom CustomData class to handle aspect_node_idx as an integer, a custom collate function for DataLoader, and functions to validate data, balance classes, and generate graphs. It processes the ABSA dataset, creates graphs with embeddings, filters invalid graphs, and saves them to pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.utils import resample\n",
    "\n",
    "class CustomData(Data):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if hasattr(self, 'aspect_node_idx') and isinstance(self.aspect_node_idx, torch.Tensor):\n",
    "            self.aspect_node_idx = self.aspect_node_idx.item()\n",
    "\n",
    "    def __setattr__(self, key, value):\n",
    "        if key == 'aspect_node_idx' and isinstance(value, torch.Tensor):\n",
    "            value = value.item()\n",
    "        super().__setattr__(key, value)\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        obj = super().to(*args, **kwargs)\n",
    "        if hasattr(obj, 'aspect_node_idx') and isinstance(obj.aspect_node_idx, torch.Tensor):\n",
    "            obj.aspect_node_idx = obj.aspect_node_idx.item()\n",
    "        return obj\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        if 'aspect_node_idx' in state and isinstance(state['aspect_node_idx'], torch.Tensor):\n",
    "            state['aspect_node_idx'] = state['aspect_node_idx'].item()\n",
    "        return state\n",
    "\n",
    "def custom_collate(batch):\n",
    "    if len(batch) != 1:\n",
    "        raise ValueError(f\"Expected batch_size=1, but got batch size {len(batch)}\")\n",
    "    context_graph, dep_graph = batch[0]\n",
    "    if isinstance(context_graph.aspect_node_idx, torch.Tensor):\n",
    "        context_graph.aspect_node_idx = context_graph.aspect_node_idx.item()\n",
    "    if isinstance(dep_graph.aspect_node_idx, torch.Tensor):\n",
    "        dep_graph.aspect_node_idx = dep_graph.aspect_node_idx.item()\n",
    "    return context_graph, dep_graph\n",
    "\n",
    "def validate_data(df, name=\"dataset\"):\n",
    "    valid_rows = []\n",
    "    for idx, row in df.iterrows():\n",
    "        aspect = normalize_tokens(row['aspect'])\n",
    "        sentence_tokens = [normalize_tokens(token.text) for token in nlp(row['sentence'])]\n",
    "        sentiment = str(row['sentiment']).lower() \n",
    "    \n",
    "        if not any(token in sentence_tokens for token in aspect.split()):\n",
    "            print(f\"Skipping row {idx} in {name}: No aspect tokens from '{aspect}' found in sentence '{row['sentence']}'\")\n",
    "            continue\n",
    "        if sentiment not in [k.lower() for k in sentiment_to_idx.keys()]:\n",
    "            print(f\"Skipping row {idx} in {name}: Invalid sentiment '{sentiment}' in sentence '{row['sentence']}'\")\n",
    "            continue\n",
    "        valid_rows.append(row)\n",
    "    return pd.DataFrame(valid_rows)\n",
    "\n",
    "sentiment_to_idx = {'positive': 0, 'negative': 1, 'neutral': 2}\n",
    "idx_to_sentiment = {idx: label for label, idx in sentiment_to_idx.items()}\n",
    "train_df = validate_data(train_df, \"train_df\")\n",
    "val_df = validate_data(val_df, \"val_df\")\n",
    "\n",
    "max_size = train_df['sentiment_label'].value_counts().max()\n",
    "balanced_dfs = []\n",
    "for label in sentiment_to_idx.values():\n",
    "    df_label = train_df[train_df['sentiment_label'] == label]\n",
    "    df_oversampled = resample(df_label, replace=True, n_samples=max_size, random_state=42)\n",
    "    balanced_dfs.append(df_oversampled)\n",
    "train_df = pd.concat(balanced_dfs)\n",
    "\n",
    "for df in [train_df, val_df]:\n",
    "    df['sentiment_label'] = df['sentiment'].str.lower().map({k.lower(): v for k, v in sentiment_to_idx.items()})\n",
    "    if df['sentiment_label'].isna().any():\n",
    "        print(f\"Warning: NaN values found in sentiment_label for {df.name if hasattr(df, 'name') else 'dataset'}\")\n",
    "        print(df[df['sentiment_label'].isna()][['sentence', 'sentiment']])\n",
    "        df = df.dropna(subset=['sentiment_label'])\n",
    "    df['sentiment_label'] = df['sentiment_label'].astype(int)\n",
    "\n",
    "print(\"Class distribution in train_df:\")\n",
    "class_counts = train_df['sentiment_label'].value_counts().sort_index()\n",
    "print(class_counts)\n",
    "print(\"Class distribution in val_df:\")\n",
    "print(val_df['sentiment_label'].value_counts().sort_index())\n",
    "if any(class_counts == 0):\n",
    "    raise ValueError(\"One or more classes have zero instances in train_df.\")\n",
    "class_weights = torch.tensor([1.0 / class_counts[i] for i in range(len(class_counts))], dtype=torch.float).to(device)\n",
    "class_weights = class_weights / class_weights.max()\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "def get_roberta_embeddings(sentence, nodes):\n",
    "    sentences = [sentence]\n",
    "    nodes_list = [nodes]\n",
    "    valid_sentences = []\n",
    "    valid_nodes = []\n",
    "    for i, (sent, node) in enumerate(zip(sentences, nodes_list)):\n",
    "        if isinstance(sent, str) and sent.strip() and node:\n",
    "            valid_sentences.append(sent)\n",
    "            valid_nodes.append(node)\n",
    "    if not valid_sentences:\n",
    "        return None, 0\n",
    "    try:\n",
    "        inputs = tokenizer(valid_sentences, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = roberta_model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.cpu()\n",
    "        num_nodes = min(len(valid_nodes[0]), embeddings[0].shape[0] - 2)\n",
    "        if num_nodes == 0:\n",
    "            return None, 0\n",
    "        emb = embeddings[0, 1:num_nodes+1].clone()\n",
    "        return emb, num_nodes\n",
    "    except RuntimeError:\n",
    "        return None, 0\n",
    "\n",
    "def create_dependency_graph(sentence, num_nodes=None):\n",
    "    doc = nlp(sentence)\n",
    "    original_nodes = [token.text.lower() for token in doc]\n",
    "    if not original_nodes:\n",
    "        return None\n",
    "    if num_nodes is None:\n",
    "        num_nodes = len(original_nodes)\n",
    "    nodes = original_nodes[:num_nodes]\n",
    "    edges = []\n",
    "    edge_types = []\n",
    "    for token in doc:\n",
    "        if token.head != token and token.i < num_nodes and token.head.i < num_nodes:\n",
    "            edges.append([token.i, token.head.i])\n",
    "            edge_types.append(dep_type_to_idx.get(token.dep_, 0))\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous() if edges else torch.empty((2, 0), dtype=torch.long)\n",
    "    edge_types = torch.tensor(edge_types, dtype=torch.long) if edge_types else torch.empty((0,), dtype=torch.long)\n",
    "    noun_indices = [i for i, token in enumerate(doc) if token.pos_ == 'NOUN' and i < num_nodes]\n",
    "    adj_indices = [i for i, token in enumerate(doc) if token.pos_ == 'ADJ' and i < num_nodes]\n",
    "    graph = CustomData(edge_index=edge_index, edge_attr=edge_types, sentence=sentence, nodes=nodes, noun_indices=noun_indices, adj_indices=adj_indices)\n",
    "    return graph\n",
    "\n",
    "\n",
    "def create_contextual_graph(sentence, num_nodes=None):\n",
    "    doc = nlp(sentence)\n",
    "    original_nodes = [token.text.lower() for token in doc]\n",
    "    if not original_nodes:\n",
    "        return None\n",
    "    if num_nodes is None:\n",
    "        num_nodes = len(original_nodes)\n",
    "    nodes = original_nodes[:num_nodes]\n",
    "    edges = []\n",
    "    window = 2\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(max(0, i - window), min(num_nodes, i + window + 1)):\n",
    "            if i != j and i < num_nodes and j < num_nodes:\n",
    "                edges.append([i, j])\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous() if edges else torch.empty((2, 0), dtype=torch.long)\n",
    "    noun_indices = [i for i, token in enumerate(doc) if token.pos_ == 'NOUN' and i < num_nodes]\n",
    "    adj_indices = [i for i, token in enumerate(doc) if token.pos_ == 'ADJ' and i < num_nodes]\n",
    "    graph = CustomData(edge_index=edge_index, sentence=sentence, nodes=nodes, noun_indices=noun_indices, adj_indices=adj_indices)\n",
    "    return graph\n",
    "\n",
    "def normalize_tokens(text):\n",
    "    return ' '.join(str(text).lower().replace('-', ' ').split())\n",
    "\n",
    "train_graphs = []\n",
    "for idx, row in train_df.iterrows():\n",
    "    dep_graph = create_dependency_graph(row['sentence'])\n",
    "    if dep_graph is None:\n",
    "        continue\n",
    "    context_graph = create_contextual_graph(row['sentence'])\n",
    "    if context_graph is None:\n",
    "        continue\n",
    "    embeddings, num_nodes = get_roberta_embeddings(row['sentence'], dep_graph.nodes)\n",
    "    if embeddings is None:\n",
    "        continue\n",
    "    dep_graph = create_dependency_graph(row['sentence'], num_nodes)\n",
    "    context_graph = create_contextual_graph(row['sentence'], num_nodes)\n",
    "    dep_graph.x = embeddings\n",
    "    context_graph.x = embeddings\n",
    "    aspect = normalize_tokens(row['aspect'])\n",
    "    aspect_tokens = aspect.split()\n",
    "    aspect_node_idx = None\n",
    "    nodes = [normalize_tokens(node) for node in dep_graph.nodes]\n",
    "    for i in range(len(nodes) - len(aspect_tokens) + 1):\n",
    "        if ' '.join(nodes[i:i + len(aspect_tokens)]) == aspect:\n",
    "            aspect_node_idx = i\n",
    "            break\n",
    "    if aspect_node_idx is None:\n",
    "        continue\n",
    "    dep_graph.aspect_node_idx = int(aspect_node_idx)\n",
    "    context_graph.aspect_node_idx = int(aspect_node_idx)\n",
    "    dep_graph.aspect = row['aspect']\n",
    "    context_graph.aspect = row['aspect']\n",
    "    dep_graph.label = row['sentiment_label']\n",
    "    context_graph.label = row['sentiment_label']\n",
    "    train_graphs.append((context_graph, dep_graph))\n",
    "\n",
    "val_graphs = []\n",
    "for idx, row in val_df.iterrows():\n",
    "    dep_graph = create_dependency_graph(row['sentence'])\n",
    "    if dep_graph is None:\n",
    "        continue\n",
    "    context_graph = create_contextual_graph(row['sentence'])\n",
    "    if context_graph is None:\n",
    "        continue\n",
    "    embeddings, num_nodes = get_roberta_embeddings(row['sentence'], dep_graph.nodes)\n",
    "    if embeddings is None:\n",
    "        continue\n",
    "    dep_graph = create_dependency_graph(row['sentence'], num_nodes)\n",
    "    context_graph = create_contextual_graph(row['sentence'], num_nodes)\n",
    "    dep_graph.x = embeddings\n",
    "    context_graph.x = embeddings\n",
    "    aspect = normalize_tokens(row['aspect'])\n",
    "    aspect_tokens = aspect.split()\n",
    "    aspect_node_idx = None\n",
    "    nodes = [normalize_tokens(node) for node in dep_graph.nodes]\n",
    "    for i in range(len(nodes) - len(aspect_tokens) + 1):\n",
    "        if ' '.join(nodes[i:i + len(aspect_tokens)]) == aspect:\n",
    "            aspect_node_idx = i\n",
    "            break\n",
    "    if aspect_node_idx is None:\n",
    "        continue\n",
    "    dep_graph.aspect_node_idx = int(aspect_node_idx)\n",
    "    context_graph.aspect_node_idx = int(aspect_node_idx)\n",
    "    dep_graph.aspect = row['aspect']\n",
    "    context_graph.aspect = row['aspect']\n",
    "    dep_graph.label = row['sentiment_label']\n",
    "    context_graph.label = row['sentiment_label']\n",
    "    val_graphs.append((context_graph, dep_graph))\n",
    "\n",
    "def is_valid_graph(c, d):\n",
    "    try:\n",
    "        if c.x.shape[0] == 0 or d.x.shape[0] == 0:\n",
    "            return False\n",
    "        if c.edge_index.numel() > 0 and c.edge_index.max() >= c.x.shape[0]:\n",
    "            return False\n",
    "        if d.edge_index.numel() > 0 and d.edge_index.max() >= d.x.shape[0]:\n",
    "            return False\n",
    "        if not isinstance(c.aspect_node_idx, int) or not isinstance(d.aspect_node_idx, int):\n",
    "            return False\n",
    "        if c.aspect_node_idx < 0 or c.aspect_node_idx >= c.x.shape[0]:\n",
    "            return False\n",
    "        if d.aspect_node_idx < 0 or d.aspect_node_idx >= d.x.shape[0]:\n",
    "            return False\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "train_graphs = [(c, d) for c, d in train_graphs if is_valid_graph(c, d)]\n",
    "val_graphs = [(c, d) for c, d in val_graphs if is_valid_graph(c, d)]\n",
    "\n",
    "with open('data/semeval_train_graphs_embedded.pkl', 'wb') as f:\n",
    "    pickle.dump(train_graphs, f)\n",
    "with open('data/semeval_val_graphs_embedded.pkl', 'wb') as f:\n",
    "    pickle.dump(val_graphs, f)\n",
    "print(f\"Total train graphs: {len(train_graphs)}, Total validation graphs: {len(val_graphs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines the HybridGCNForABSA model for aspect-based sentiment analysis, combining T-GCN and ARGCN with a classification head. It loads pre-trained weights, creates DataLoaders with a custom collate function, and fine-tunes the model with early stopping and checkpointing, saving the final model to finetuned_gcn_200_clean.ckpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HybridGCNForABSA(pl.LightningModule):\n",
    "    def __init__(self, input_dim=768, hidden_dim=256, num_dep_types=50, num_classes=3, class_weights=None):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.tgcn_conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.tgcn_conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.tgcn_dropout = nn.Dropout(0.3)\n",
    "        self.argcn_conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.argcn_conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.argcn_dropout = nn.Dropout(0.3)\n",
    "        self.type_emb = nn.Embedding(num_dep_types, hidden_dim)\n",
    "        self.fc_sentiment = nn.Linear(hidden_dim * 2, num_classes)\n",
    "        self.dep_type_to_idx = dep_type_to_idx\n",
    "        self.criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        self.val_preds = []\n",
    "        self.val_labels = []\n",
    "        self.train_preds = []\n",
    "        self.train_labels = []\n",
    "\n",
    "    def forward(self, context_data, dep_data, target_idx=None):\n",
    "        x_tgcn = context_data.x.to(device)\n",
    "        edge_index_tgcn = context_data.edge_index.to(device)\n",
    "        x_argcn = dep_data.x.to(device)\n",
    "        edge_index_argcn = dep_data.edge_index.to(device)\n",
    "        edge_attr = dep_data.edge_attr.to(device) if dep_data.edge_attr is not None else None\n",
    "\n",
    "        num_nodes_tgcn = x_tgcn.shape[0]\n",
    "        num_nodes_argcn = x_argcn.shape[0]\n",
    "        if edge_index_tgcn.numel() > 0 and edge_index_tgcn.max().item() >= num_nodes_tgcn:\n",
    "            raise ValueError(f\"Invalid edge_index_tgcn: max index {edge_index_tgcn.max().item()}, num_nodes {num_nodes_tgcn}\")\n",
    "        if edge_index_argcn.numel() > 0 and edge_index_argcn.max().item() >= num_nodes_argcn:\n",
    "            raise ValueError(f\"Invalid edge_index_argcn: max index {edge_index_argcn.max().item()}, num_nodes {num_nodes_argcn}\")\n",
    "        if target_idx is None or target_idx >= num_nodes_tgcn or target_idx >= num_nodes_argcn:\n",
    "            raise ValueError(f\"Invalid target_idx: {target_idx}, num_nodes_tgcn={num_nodes_tgcn}, num_nodes_argcn={num_nodes_argcn}\")\n",
    "\n",
    "        x_tgcn1 = torch.relu(self.tgcn_conv1(x_tgcn, edge_index_tgcn))\n",
    "        x_tgcn1 = self.tgcn_dropout(x_tgcn1)\n",
    "        x_tgcn2 = torch.relu(self.tgcn_conv2(x_tgcn1, edge_index_tgcn))\n",
    "        x_tgcn = (x_tgcn1 + x_tgcn2) / 2\n",
    "\n",
    "        edge_weights = torch.ones(edge_index_argcn.size(1), device=device)\n",
    "        if edge_attr is not None:\n",
    "            valid_mask = edge_attr != -1\n",
    "            valid_edge_attr = edge_attr[valid_mask]\n",
    "            if valid_edge_attr.numel() > 0:\n",
    "                edge_weights[valid_mask] = self.type_emb(valid_edge_attr).mean(dim=1)\n",
    "        x_argcn1 = torch.relu(self.argcn_conv1(x_argcn, edge_index_argcn, edge_weights))\n",
    "        x_argcn1 = self.argcn_dropout(x_argcn1)\n",
    "        x_argcn2 = torch.relu(self.argcn_conv2(x_argcn1, edge_index_argcn, edge_weights))\n",
    "        x_argcn = (x_argcn1 + x_argcn2) / 2\n",
    "\n",
    "        x_tgcn_aspect = x_tgcn[target_idx].unsqueeze(0)\n",
    "        x_argcn_aspect = x_argcn[target_idx].unsqueeze(0)\n",
    "        x_combined = torch.cat([x_tgcn_aspect, x_argcn_aspect], dim=-1)\n",
    "        sentiment_logits = self.fc_sentiment(x_combined)\n",
    "        return sentiment_logits\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        self.train_preds = []\n",
    "        self.train_labels = []\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        self.val_preds = []\n",
    "        self.val_labels = []\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        context_data, dep_data = batch\n",
    "        context_data = context_data.to(device)\n",
    "        dep_data = dep_data.to(device)\n",
    "        loss = torch.tensor(0.0, device=device, requires_grad=True)\n",
    "\n",
    "        try:\n",
    "         \n",
    "            if context_data.x.shape[0] == 0 or dep_data.x.shape[0] == 0:\n",
    "                return None\n",
    "            \n",
    "          \n",
    "            if context_data.edge_index.numel() > 0 and context_data.edge_index.max().item() >= context_data.x.shape[0]:\n",
    "                return None\n",
    "            if dep_data.edge_index.numel() > 0 and dep_data.edge_index.max().item() >= dep_data.x.shape[0]:\n",
    "                return None\n",
    "\n",
    "            target_idx = context_data.aspect_node_idx\n",
    "            if isinstance(target_idx, torch.Tensor):\n",
    "                if target_idx.numel() == 1:\n",
    "                    target_idx = target_idx.item()\n",
    "                else:\n",
    "                    return None\n",
    "            elif not isinstance(target_idx, int):\n",
    "                return None\n",
    "\n",
    "        \n",
    "            label = torch.tensor([context_data.label], device=device, dtype=torch.long)\n",
    "\n",
    "           \n",
    "            logits = self(context_data, dep_data, target_idx=target_idx)\n",
    "            batch_loss = self.criterion(logits, label)\n",
    "            loss = loss + batch_loss\n",
    "            pred = torch.argmax(logits, dim=1).cpu().item()\n",
    "            self.train_preds.append(pred)\n",
    "            self.train_labels.append(label.cpu().item())\n",
    "            self.log('train_loss', batch_loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=1)\n",
    "            return loss\n",
    "\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        context_data, dep_data = batch\n",
    "        context_data = context_data.to(device)\n",
    "        dep_data = dep_data.to(device)\n",
    "\n",
    "        try:\n",
    "        \n",
    "            if context_data.x.shape[0] == 0 or dep_data.x.shape[0] == 0:\n",
    "                return\n",
    "            \n",
    "            \n",
    "            if context_data.edge_index.numel() > 0 and context_data.edge_index.max().item() >= context_data.x.shape[0]:\n",
    "                return\n",
    "            if dep_data.edge_index.numel() > 0 and dep_data.edge_index.max().item() >= dep_data.x.shape[0]:\n",
    "                return\n",
    "\n",
    "      \n",
    "            target_idx = context_data.aspect_node_idx\n",
    "            if isinstance(target_idx, torch.Tensor):\n",
    "                if target_idx.numel() == 1:\n",
    "                    target_idx = target_idx.item()\n",
    "                else:\n",
    "                    return\n",
    "            elif not isinstance(target_idx, int):\n",
    "                return\n",
    "\n",
    "            label = int(context_data.label.item())\n",
    "\n",
    "            logits = self(context_data, dep_data, target_idx=target_idx)\n",
    "            pred = torch.argmax(logits, dim=1).cpu().item()\n",
    "\n",
    "            self.val_preds.append(pred)\n",
    "            self.val_labels.append(label)\n",
    "\n",
    "        except Exception:\n",
    "            return\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        if self.train_preds and self.train_labels:\n",
    "            train_accuracy = accuracy_score(self.train_labels, self.train_preds)\n",
    "            train_f1 = f1_score(self.train_labels, self.train_preds, average='macro')\n",
    "            self.log('train_accuracy', train_accuracy, on_epoch=True, prog_bar=True)\n",
    "            self.log('train_f1', train_f1, on_epoch=True, prog_bar=True)\n",
    "            print(f\"Training Accuracy: {train_accuracy:.4f}, F1-Score: {train_f1:.4f}\")\n",
    "        self.train_preds = []\n",
    "        self.train_labels = []\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if not self.val_preds or not self.val_labels:\n",
    "            self.log('val_accuracy', 0.0, on_epoch=True, prog_bar=True)\n",
    "            self.log('val_f1', 0.0, on_epoch=True, prog_bar=True)\n",
    "            print(\"Validation Accuracy: 0.0000, F1-Score: 0.0000\")\n",
    "        else:\n",
    "            accuracy = accuracy_score(self.val_labels, self.val_preds)\n",
    "            f1 = f1_score(self.val_labels, self.val_preds, average='macro')\n",
    "            self.log('val_accuracy', accuracy, on_epoch=True, prog_bar=True)\n",
    "            self.log('val_f1', f1, on_epoch=True, prog_bar=True)\n",
    "            print(f\"Validation Accuracy: {accuracy:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "        self.val_preds = []\n",
    "        self.val_labels = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=5e-5)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell initializes the `HybridGCNForABSA` model, loads pre-trained weights from `pretrained_gcn_1.ckpt` (excluding node and edge prediction layers), and creates DataLoaders for training and validation graphs. It fine-tunes the model for 15 epochs with early stopping based on validation accuracy, saving the best model to `finetuned_gcn_200_clean.ckpt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in train_df:\n",
      "0    99\n",
      "1    99\n",
      "2    99\n",
      "Name: sentiment_label, dtype: int64\n",
      "Class distribution in val_df:\n",
      "0    20\n",
      "1     8\n",
      "2    12\n",
      "Name: sentiment_label, dtype: int64\n",
      "Class weights: tensor([1., 1., 1.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "You are using a CUDA device ('NVIDIA L40S') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | tgcn_conv1    | GCNConv          | 196 K \n",
      "1 | tgcn_conv2    | GCNConv          | 65.8 K\n",
      "2 | tgcn_dropout  | Dropout          | 0     \n",
      "3 | argcn_conv1   | GCNConv          | 196 K \n",
      "4 | argcn_conv2   | GCNConv          | 65.8 K\n",
      "5 | argcn_dropout | Dropout          | 0     \n",
      "6 | type_emb      | Embedding        | 11.3 K\n",
      "7 | fc_sentiment  | Linear           | 1.5 K \n",
      "8 | criterion     | CrossEntropyLoss | 0     \n",
      "---------------------------------------------------\n",
      "538 K     Trainable params\n",
      "0         Non-trainable params\n",
      "538 K     Total params\n",
      "2.152     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train graphs: 292, Total validation graphs: 40\n",
      "Loaded pre-trained weights from 'data/pretrained_gcn_200_clean.ckpt'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d8fe2eb9f34789aae42ff7026d7c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5000, F1-Score: 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f18ee6237c340589b7afd70a660a954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c711d0a82ff4e9f8c223f9ab32f05d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5750, F1-Score: 0.4750\n",
      "Training Accuracy: 0.3904, F1-Score: 0.3414\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584a058bd3ad4303bea914a0f350f47c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8750, F1-Score: 0.8628\n",
      "Training Accuracy: 0.6130, F1-Score: 0.6089\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1de44f6ef441bbbd28650fa6bdb169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8250, F1-Score: 0.8111\n",
      "Training Accuracy: 0.7979, F1-Score: 0.7935\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1854e90fbac14e3b892052050d65c13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8500, F1-Score: 0.8330\n",
      "Training Accuracy: 0.8253, F1-Score: 0.8246\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e36ca1586748a19c70613fc6f52673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8500, F1-Score: 0.8330\n",
      "Training Accuracy: 0.8801, F1-Score: 0.8796\n",
      "Fine-tuning completed. Checkpoint saved at: data/finetuned_gcn_Final_clean.ckpt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    model = HybridGCNForABSA(num_dep_types=len(dep_types), num_classes=3, class_weights=class_weights)\n",
    "    model.dep_type_to_idx = dep_type_to_idx\n",
    "    checkpoint = torch.load('data/pretrained_gcn_1.ckpt', map_location=device)\n",
    "    state_dict = checkpoint['state_dict']\n",
    "    state_dict = {k: v for k, v in state_dict.items() if not k.startswith('fc_node') and not k.startswith('fc_edge')}\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    print(\"Loaded pre-trained weights from 'data/pretrained_gcn_200_clean.ckpt'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load pre-trained model: {e}\")\n",
    "    raise e\n",
    "\n",
    "train_loader = DataLoader(train_graphs, batch_size=1, shuffle=True, num_workers=0, pin_memory=True, collate_fn=custom_collate)\n",
    "val_loader = DataLoader(val_graphs, batch_size=1, shuffle=False, num_workers=0, pin_memory=True, collate_fn=custom_collate)\n",
    "\n",
    "try:\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=15,\n",
    "        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "        devices=1,\n",
    "        log_every_n_steps=5,\n",
    "        enable_checkpointing=True,\n",
    "        default_root_dir='checkpoints/',\n",
    "        val_check_interval=1.0,\n",
    "        limit_val_batches=1.0,\n",
    "        callbacks=[\n",
    "            pl.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, mode='max', min_delta=0.005),\n",
    "            pl.callbacks.ModelCheckpoint(monitor='val_accuracy', mode='max', save_top_k=1)\n",
    "        ]\n",
    "    )\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    trainer.save_checkpoint('data/finetuned_gcn_200_clean.ckpt')\n",
    "    print(\"Fine-tuning completed. Checkpoint saved at: data/finetuned_gcn_Final_clean.ckpt\")\n",
    "except Exception as e:\n",
    "    print(f\"Fine-tuning failed: {e}\")\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
